# avito_contest
Репозиторий с решением отбора на стажировку Data Science в Авито

## Задача и ключевые проблемы
По условию нужно было, используя любой алгоритм или модель, расставить в текстах датасета пробелы, получив максимальный F1-score

При анализе задачи я выделил для себя такие особенности:
* датасет содержит много специфических слов на русском и английском (примеры - "Xbox One", "айфон", "ксяоми", "LG Oled" и т.д.)
* в данных много знаков препинания ("?", "-", ".", "," и т.д.) + часто использовался дефис вместо тире
* есть russian-specific слова, связанные, например, с географией ("Подмосковье" и др.)
* API моделей использовать нельзя => имеющийся небольшой датасет мы еще и корректно сами не разметим, чтобы что-то обучить

## Описание выбора модели

После короткого визуального анализа возникла идея использовать небольшую инструктивную LLM as-is, вот почему
* опенсорсные датасеты конкретно под марки разных товаров на русском и английском не существуют, сложно что-то обучить/дообучить с хорошим качеством
* создание словаря под них = практически ручная разметка (а еще это провалится на приватном датасете)
* опенсорсные токенизаторы также теряют в качестве из-за описанных выше проблем
* взять mini-LLM и дообучить - дорого и долго (а еще - на чем учить? оригинальный датасет небольшой, открытые - не совсем релевантны)

Поэтому я выбрал Vikhrmodels/QVikhr-3-4B-Instruction, потому что:
* дообучена под русский - меньше проблем
* при обучении "видела" много данных на русском и английском
* легко помещается на 2xT4 GPU (я использовал ресурсы kaggle)
* инструктивная
* хороший рейтинг по количеству скачиваний на HF (найти просто лидерборд я безуспешно пытался, но такие малые модели на ру-бенчмарках особо не измеряют)

## Эксперименты

Первая идея - просто сделать промпт на русском (с некоторым описанием правил языка) и использовать, как zero-shot. Температура 0. Своеобразный бейзлайн

Итог - **0.65 по F1**

Вторая идея:
* промпт на английском с указанием никак не менять/удалять никакие знаки препинания и буквы
* ненулевая температура
* даем модели до 20 попыток
* следим, вдруг модель поставила 0 пробелов, изменила какие-то символы или написала что-то типа "Я не могу выполнить вашу просьбу, потому что..."
* все это отслеживаем через условия, если ничего этого нет - завершаем попытки и отдаем результат
* в большинстве случаев хватает 1-2 итераций - алгоритм работает быстро в среднем

Итог - **0.804 по F1**

Третья идея:
* промпт и парамтеры модели те же, что в прошлой
* даем модели меньше попыток на то, чтобы сгенерировать что-то, подходящее под вышесказаные условия
* но даем еще 3 попытки улучшить свой результат (если в тексте стоит хоть немного пробелов, модели проще)

Итог - **0.73 по F1**

Вывод: вторая идея - лучшая. Третья, вероятно, провалилась из-за уменьшенного числа попыток. Первая была слишком тривиальной




